{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "66e3400d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c91bd001",
   "metadata": {},
   "source": [
    "## Reading the input data csv files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "id": "61fe5f34",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('Train.csv')\n",
    "df_test = pd.read_csv('Test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "id": "e44d3f03",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.135</td>\n",
       "      <td>1.0590</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.2630</td>\n",
       "      <td>0.274</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.6860</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.220</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1.5830</td>\n",
       "      <td>0.7395</td>\n",
       "      <td>0.3505</td>\n",
       "      <td>0.405</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.8085</td>\n",
       "      <td>0.2345</td>\n",
       "      <td>0.1155</td>\n",
       "      <td>0.350</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   A      B      C      D       E       F       G      H  Target\n",
       "0  F  0.615  0.455  0.135  1.0590  0.4735  0.2630  0.274       9\n",
       "1  F  0.515  0.395  0.140  0.6860  0.2810  0.1255  0.220      12\n",
       "2  M  0.660  0.530  0.175  1.5830  0.7395  0.3505  0.405      10\n",
       "3  M  0.455  0.365  0.095  0.5140  0.2245  0.1010  0.150      15\n",
       "4  M  0.495  0.400  0.155  0.8085  0.2345  0.1155  0.350       6"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "a79e3c0f",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>866</td>\n",
       "      <td>M</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.1035</td>\n",
       "      <td>0.4210</td>\n",
       "      <td>0.3015</td>\n",
       "      <td>0.325</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1483</td>\n",
       "      <td>M</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.8725</td>\n",
       "      <td>0.3870</td>\n",
       "      <td>0.2150</td>\n",
       "      <td>0.245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>599</td>\n",
       "      <td>F</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.9810</td>\n",
       "      <td>0.3050</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.335</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1702</td>\n",
       "      <td>F</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.2615</td>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.2665</td>\n",
       "      <td>0.380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>670</td>\n",
       "      <td>M</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.6175</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.215</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index  A      B      C      D       E       F       G      H\n",
       "0    866  M  0.605  0.455  0.160  1.1035  0.4210  0.3015  0.325\n",
       "1   1483  M  0.590  0.440  0.150  0.8725  0.3870  0.2150  0.245\n",
       "2    599  F  0.560  0.445  0.195  0.9810  0.3050  0.2245  0.335\n",
       "3   1702  F  0.635  0.490  0.170  1.2615  0.5385  0.2665  0.380\n",
       "4    670  M  0.475  0.385  0.145  0.6175  0.2350  0.1080  0.215"
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "id": "53b7e002",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "A         0\n",
       "B         0\n",
       "C         0\n",
       "D         0\n",
       "E         0\n",
       "F         0\n",
       "G         0\n",
       "H         0\n",
       "Target    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.isnull().sum() \n",
    "\n",
    "### No null values in the given dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "id": "31d2caf3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_train.duplicated().sum()\n",
    "## No duplicates "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b59beaf",
   "metadata": {},
   "source": [
    "- **The dataset is clean**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "id": "69628b07",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 1152x576 with 8 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "## checking for outliers\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_outliers(dataframe):\n",
    "    fig, axes = plt.subplots(2, 4, figsize=(16, 8))\n",
    "    fig.tight_layout(pad=5.0)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i, column in enumerate(dataframe.columns[:7]):  \n",
    "        sns.boxplot(x=dataframe[column], ax=axes[i])\n",
    "        axes[i].set_title(column)\n",
    "    axes[-1].axis('off')\n",
    "    plt.show()\n",
    "\n",
    "df = df_train.drop(['A','Target'], axis = 1)\n",
    "plot_outliers(df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "id": "7666dd21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def remove_outliers(dataframe, columns):\n",
    "    df = dataframe.copy()\n",
    "    for column in columns:\n",
    "        Q1 = df[column].quantile(0.25)\n",
    "        Q3 = df[column].quantile(0.75)\n",
    "        IQR = Q3 - Q1\n",
    "        lower_bound = Q1 - 1.5 * IQR\n",
    "        upper_bound = Q3 + 1.5 * IQR\n",
    "        \n",
    "        df = df[(df[column] >= lower_bound) & (df[column] <= upper_bound)]\n",
    "\n",
    "    return df\n",
    "\n",
    "df = remove_outliers(df_train, ['B','C','D','E','F','G','H'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "id": "5e542bd1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>A</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>Target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>F</td>\n",
       "      <td>0.615</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.135</td>\n",
       "      <td>1.0590</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.2630</td>\n",
       "      <td>0.274</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>F</td>\n",
       "      <td>0.515</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.6860</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.220</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>M</td>\n",
       "      <td>0.660</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1.5830</td>\n",
       "      <td>0.7395</td>\n",
       "      <td>0.3505</td>\n",
       "      <td>0.405</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>M</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>M</td>\n",
       "      <td>0.495</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.8085</td>\n",
       "      <td>0.2345</td>\n",
       "      <td>0.1155</td>\n",
       "      <td>0.350</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127</th>\n",
       "      <td>F</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.2560</td>\n",
       "      <td>0.1325</td>\n",
       "      <td>0.145</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3128</th>\n",
       "      <td>F</td>\n",
       "      <td>0.670</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.190</td>\n",
       "      <td>1.3905</td>\n",
       "      <td>0.5425</td>\n",
       "      <td>0.3035</td>\n",
       "      <td>0.400</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3129</th>\n",
       "      <td>M</td>\n",
       "      <td>0.510</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5805</td>\n",
       "      <td>0.2440</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.188</td>\n",
       "      <td>11</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>M</td>\n",
       "      <td>0.575</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.120</td>\n",
       "      <td>1.0535</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2185</td>\n",
       "      <td>0.235</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3131</th>\n",
       "      <td>F</td>\n",
       "      <td>0.595</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.1405</td>\n",
       "      <td>0.5470</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.271</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3023 rows × 9 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      A      B      C      D       E       F       G      H  Target\n",
       "0     F  0.615  0.455  0.135  1.0590  0.4735  0.2630  0.274       9\n",
       "1     F  0.515  0.395  0.140  0.6860  0.2810  0.1255  0.220      12\n",
       "2     M  0.660  0.530  0.175  1.5830  0.7395  0.3505  0.405      10\n",
       "3     M  0.455  0.365  0.095  0.5140  0.2245  0.1010  0.150      15\n",
       "4     M  0.495  0.400  0.155  0.8085  0.2345  0.1155  0.350       6\n",
       "...  ..    ...    ...    ...     ...     ...     ...    ...     ...\n",
       "3127  F  0.490  0.400  0.115  0.5690  0.2560  0.1325  0.145       9\n",
       "3128  F  0.670  0.550  0.190  1.3905  0.5425  0.3035  0.400      12\n",
       "3129  M  0.510  0.395  0.125  0.5805  0.2440  0.1335  0.188      11\n",
       "3130  M  0.575  0.465  0.120  1.0535  0.5160  0.2185  0.235       9\n",
       "3131  F  0.595  0.475  0.160  1.1405  0.5470  0.2310  0.271       6\n",
       "\n",
       "[3023 rows x 9 columns]"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1db553ad",
   "metadata": {},
   "source": [
    "### Column A has categorical values. since its a nominal data One-Hot Encoding is applied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "id": "d446eb66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>Target</th>\n",
       "      <th>gender_F</th>\n",
       "      <th>gender_I</th>\n",
       "      <th>gender_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.615</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.135</td>\n",
       "      <td>1.0590</td>\n",
       "      <td>0.4735</td>\n",
       "      <td>0.2630</td>\n",
       "      <td>0.274</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.515</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.140</td>\n",
       "      <td>0.6860</td>\n",
       "      <td>0.2810</td>\n",
       "      <td>0.1255</td>\n",
       "      <td>0.220</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.660</td>\n",
       "      <td>0.530</td>\n",
       "      <td>0.175</td>\n",
       "      <td>1.5830</td>\n",
       "      <td>0.7395</td>\n",
       "      <td>0.3505</td>\n",
       "      <td>0.405</td>\n",
       "      <td>10</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.455</td>\n",
       "      <td>0.365</td>\n",
       "      <td>0.095</td>\n",
       "      <td>0.5140</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.1010</td>\n",
       "      <td>0.150</td>\n",
       "      <td>15</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.495</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.155</td>\n",
       "      <td>0.8085</td>\n",
       "      <td>0.2345</td>\n",
       "      <td>0.1155</td>\n",
       "      <td>0.350</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3127</th>\n",
       "      <td>0.490</td>\n",
       "      <td>0.400</td>\n",
       "      <td>0.115</td>\n",
       "      <td>0.5690</td>\n",
       "      <td>0.2560</td>\n",
       "      <td>0.1325</td>\n",
       "      <td>0.145</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3128</th>\n",
       "      <td>0.670</td>\n",
       "      <td>0.550</td>\n",
       "      <td>0.190</td>\n",
       "      <td>1.3905</td>\n",
       "      <td>0.5425</td>\n",
       "      <td>0.3035</td>\n",
       "      <td>0.400</td>\n",
       "      <td>12</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3129</th>\n",
       "      <td>0.510</td>\n",
       "      <td>0.395</td>\n",
       "      <td>0.125</td>\n",
       "      <td>0.5805</td>\n",
       "      <td>0.2440</td>\n",
       "      <td>0.1335</td>\n",
       "      <td>0.188</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3130</th>\n",
       "      <td>0.575</td>\n",
       "      <td>0.465</td>\n",
       "      <td>0.120</td>\n",
       "      <td>1.0535</td>\n",
       "      <td>0.5160</td>\n",
       "      <td>0.2185</td>\n",
       "      <td>0.235</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3131</th>\n",
       "      <td>0.595</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.1405</td>\n",
       "      <td>0.5470</td>\n",
       "      <td>0.2310</td>\n",
       "      <td>0.271</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3023 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          B      C      D       E       F       G      H  Target  gender_F  \\\n",
       "0     0.615  0.455  0.135  1.0590  0.4735  0.2630  0.274       9         1   \n",
       "1     0.515  0.395  0.140  0.6860  0.2810  0.1255  0.220      12         1   \n",
       "2     0.660  0.530  0.175  1.5830  0.7395  0.3505  0.405      10         0   \n",
       "3     0.455  0.365  0.095  0.5140  0.2245  0.1010  0.150      15         0   \n",
       "4     0.495  0.400  0.155  0.8085  0.2345  0.1155  0.350       6         0   \n",
       "...     ...    ...    ...     ...     ...     ...    ...     ...       ...   \n",
       "3127  0.490  0.400  0.115  0.5690  0.2560  0.1325  0.145       9         1   \n",
       "3128  0.670  0.550  0.190  1.3905  0.5425  0.3035  0.400      12         1   \n",
       "3129  0.510  0.395  0.125  0.5805  0.2440  0.1335  0.188      11         0   \n",
       "3130  0.575  0.465  0.120  1.0535  0.5160  0.2185  0.235       9         0   \n",
       "3131  0.595  0.475  0.160  1.1405  0.5470  0.2310  0.271       6         1   \n",
       "\n",
       "      gender_I  gender_M  \n",
       "0            0         0  \n",
       "1            0         0  \n",
       "2            0         1  \n",
       "3            0         1  \n",
       "4            0         1  \n",
       "...        ...       ...  \n",
       "3127         0         0  \n",
       "3128         0         0  \n",
       "3129         0         1  \n",
       "3130         0         1  \n",
       "3131         0         0  \n",
       "\n",
       "[3023 rows x 11 columns]"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.get_dummies(df, prefix='gender')\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "id": "250a93da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Index</th>\n",
       "      <th>B</th>\n",
       "      <th>C</th>\n",
       "      <th>D</th>\n",
       "      <th>E</th>\n",
       "      <th>F</th>\n",
       "      <th>G</th>\n",
       "      <th>H</th>\n",
       "      <th>gender_F</th>\n",
       "      <th>gender_I</th>\n",
       "      <th>gender_M</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>866</td>\n",
       "      <td>0.605</td>\n",
       "      <td>0.455</td>\n",
       "      <td>0.160</td>\n",
       "      <td>1.1035</td>\n",
       "      <td>0.4210</td>\n",
       "      <td>0.3015</td>\n",
       "      <td>0.325</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1483</td>\n",
       "      <td>0.590</td>\n",
       "      <td>0.440</td>\n",
       "      <td>0.150</td>\n",
       "      <td>0.8725</td>\n",
       "      <td>0.3870</td>\n",
       "      <td>0.2150</td>\n",
       "      <td>0.245</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>599</td>\n",
       "      <td>0.560</td>\n",
       "      <td>0.445</td>\n",
       "      <td>0.195</td>\n",
       "      <td>0.9810</td>\n",
       "      <td>0.3050</td>\n",
       "      <td>0.2245</td>\n",
       "      <td>0.335</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1702</td>\n",
       "      <td>0.635</td>\n",
       "      <td>0.490</td>\n",
       "      <td>0.170</td>\n",
       "      <td>1.2615</td>\n",
       "      <td>0.5385</td>\n",
       "      <td>0.2665</td>\n",
       "      <td>0.380</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>670</td>\n",
       "      <td>0.475</td>\n",
       "      <td>0.385</td>\n",
       "      <td>0.145</td>\n",
       "      <td>0.6175</td>\n",
       "      <td>0.2350</td>\n",
       "      <td>0.1080</td>\n",
       "      <td>0.215</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Index      B      C      D       E       F       G      H  gender_F  \\\n",
       "0    866  0.605  0.455  0.160  1.1035  0.4210  0.3015  0.325         0   \n",
       "1   1483  0.590  0.440  0.150  0.8725  0.3870  0.2150  0.245         0   \n",
       "2    599  0.560  0.445  0.195  0.9810  0.3050  0.2245  0.335         1   \n",
       "3   1702  0.635  0.490  0.170  1.2615  0.5385  0.2665  0.380         1   \n",
       "4    670  0.475  0.385  0.145  0.6175  0.2350  0.1080  0.215         0   \n",
       "\n",
       "   gender_I  gender_M  \n",
       "0         0         1  \n",
       "1         0         1  \n",
       "2         0         0  \n",
       "3         0         0  \n",
       "4         0         1  "
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##  It's also applied to TEST  \n",
    "df_test = pd.get_dummies(df_test, prefix='gender')\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "955a625f",
   "metadata": {},
   "source": [
    "## Splitting into validation and traning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "d4549062",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X = df.drop('Target',axis= 1)\n",
    "Y =df['Target']\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(X, Y, test_size=0.2, random_state=243)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "id": "beefec55",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Scaling the features before splitting the data\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "X = scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "id": "a3e0482c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X, Y, test_size=0.2, random_state=243)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96d27ad3",
   "metadata": {},
   "source": [
    "## Logistic Regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "860870ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "logreg = LogisticRegression()\n",
    "logreg = LogisticRegression(max_iter=1000) \n",
    "logreg = LogisticRegression(solver='liblinear')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "011f913e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-6 {color: black;background-color: white;}#sk-container-id-6 pre{padding: 0;}#sk-container-id-6 div.sk-toggleable {background-color: white;}#sk-container-id-6 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-6 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-6 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-6 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-6 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-6 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-6 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-6 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-6 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-6 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-6 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-6 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-6 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-6 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-6 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-6 div.sk-item {position: relative;z-index: 1;}#sk-container-id-6 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-6 div.sk-item::before, #sk-container-id-6 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-6 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-6 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-6 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-6 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-6 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-6 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-6 div.sk-label-container {text-align: center;}#sk-container-id-6 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-6 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-6\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-6\" type=\"checkbox\" checked><label for=\"sk-estimator-id-6\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "id": "229ab332",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "b0c54dc2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.22479338842975208\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       1.00      0.00      0.00         2\n",
      "           4       1.00      0.00      0.00         6\n",
      "           5       0.20      0.08      0.12        12\n",
      "           6       0.30      0.36      0.33        39\n",
      "           7       0.25      0.17      0.20        58\n",
      "           8       0.26      0.39      0.31        87\n",
      "           9       0.28      0.39      0.33       110\n",
      "          10       0.16      0.31      0.22        80\n",
      "          11       0.15      0.13      0.14        70\n",
      "          12       0.00      0.00      0.00        36\n",
      "          13       0.00      0.00      0.00        29\n",
      "          14       1.00      0.00      0.00        19\n",
      "          15       1.00      0.00      0.00        14\n",
      "          16       1.00      0.00      0.00        11\n",
      "          17       1.00      0.00      0.00        10\n",
      "          18       1.00      0.00      0.00         5\n",
      "          19       1.00      0.00      0.00         4\n",
      "          20       0.00      0.00      0.00         8\n",
      "          21       1.00      0.00      0.00         3\n",
      "          22       1.00      0.00      0.00         1\n",
      "          23       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.22       605\n",
      "   macro avg       0.60      0.09      0.08       605\n",
      "weighted avg       0.30      0.22      0.19       605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred,zero_division=1)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "id": "998c382f",
   "metadata": {},
   "outputs": [],
   "source": [
    "### tuning the hyperparameters of logistic regression\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "def tune_logistic_regression(X, y):\n",
    "    # Define the hyperparameters to tune\n",
    "    param_grid = {\n",
    "        'penalty': ['l1', 'l2'],\n",
    "        'C': [0.1, 1, 10],\n",
    "        'solver': ['liblinear', 'lbfgs'],\n",
    "        'max_iter': [100, 200, 500]\n",
    "    }\n",
    "\n",
    "    # Create the logistic regression model\n",
    "    logreg = LogisticRegression()\n",
    "    logreg = LogisticRegression(max_iter=1000)\n",
    "    logreg = LogisticRegression(solver='liblinear')\n",
    "\n",
    "\n",
    "    # Perform grid search with cross-validation\n",
    "    grid_search = GridSearchCV(logreg, param_grid, cv=5, scoring='accuracy')\n",
    "    grid_search.fit(X, y)\n",
    "\n",
    "    # Print the best hyperparameters and corresponding accuracy score\n",
    "    print(\"Best Hyperparameters: \", grid_search.best_params_)\n",
    "    print(\"Best Accuracy: \", grid_search.best_score_)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "id": "677cfa41",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python38\\lib\\site-packages\\sklearn\\model_selection\\_split.py:700: UserWarning: The least populated class in y has only 1 members, which is less than n_splits=5.\n",
      "  warnings.warn(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\svm\\_base.py:1244: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  warnings.warn(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py:458: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py:378: FitFailedWarning: \n",
      "45 fits failed out of a total of 180.\n",
      "The score on these train-test partitions for these parameters will be set to nan.\n",
      "If these failures are not expected, you can try to debug them by setting error_score='raise'.\n",
      "\n",
      "Below are more details about the failures:\n",
      "--------------------------------------------------------------------------------\n",
      "45 fits failed with the following error:\n",
      "Traceback (most recent call last):\n",
      "  File \"c:\\python38\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\", line 686, in _fit_and_score\n",
      "    estimator.fit(X_train, y_train, **fit_params)\n",
      "  File \"c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 1162, in fit\n",
      "    solver = _check_solver(self.solver, self.penalty, self.dual)\n",
      "  File \"c:\\python38\\lib\\site-packages\\sklearn\\linear_model\\_logistic.py\", line 54, in _check_solver\n",
      "    raise ValueError(\n",
      "ValueError: Solver lbfgs supports only 'l2' or 'none' penalties, got l1 penalty.\n",
      "\n",
      "  warnings.warn(some_fits_failed_message, FitFailedWarning)\n",
      "c:\\python38\\lib\\site-packages\\sklearn\\model_selection\\_search.py:952: UserWarning: One or more of the test scores are non-finite: [0.24856185        nan 0.24360317 0.25352908 0.24856185        nan\n",
      " 0.24360317 0.25352908 0.24856185        nan 0.24360317 0.25352908\n",
      " 0.25393888        nan 0.25476618 0.25973085 0.25393888        nan\n",
      " 0.25476618 0.26096966 0.25393888        nan 0.25476618 0.26179782\n",
      " 0.25311329        nan 0.25393888 0.25766217 0.25311329        nan\n",
      " 0.25393888 0.25931591 0.25311329        nan 0.25393888 0.2593142 ]\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Hyperparameters:  {'C': 1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
      "Best Accuracy:  0.26179782009821534\n"
     ]
    }
   ],
   "source": [
    "tune_logistic_regression(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "id": "2fc5b955",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Best Hyperparameters:  {'C': 1, 'max_iter': 500, 'penalty': 'l2', 'solver': 'lbfgs'}\n",
    "## Best Accuracy:  0.26179782009821534"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "7507ff08",
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(C=1, max_iter=500, penalty = 'l2', solver='lbfgs')\n",
    "logreg = LogisticRegression(max_iter=1000) \n",
    "logreg = LogisticRegression(solver='liblinear')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "id": "13bb932f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-7 {color: black;background-color: white;}#sk-container-id-7 pre{padding: 0;}#sk-container-id-7 div.sk-toggleable {background-color: white;}#sk-container-id-7 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-7 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-7 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-7 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-7 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-7 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-7 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-7 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-7 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-7 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-7 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-7 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-7 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-7 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-7 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-7 div.sk-item {position: relative;z-index: 1;}#sk-container-id-7 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-7 div.sk-item::before, #sk-container-id-7 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-7 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-7 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-7 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-7 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-7 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-7 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-7 div.sk-label-container {text-align: center;}#sk-container-id-7 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-7 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-7\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" checked><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">LogisticRegression</label><div class=\"sk-toggleable__content\"><pre>LogisticRegression(solver=&#x27;liblinear&#x27;)</pre></div></div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression(solver='liblinear')"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "logreg.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "id": "988f36af",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = logreg.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "1731a6fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.22479338842975208\n",
      "Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           3       1.00      0.00      0.00         2\n",
      "           4       1.00      0.00      0.00         6\n",
      "           5       0.20      0.08      0.12        12\n",
      "           6       0.30      0.36      0.33        39\n",
      "           7       0.25      0.17      0.20        58\n",
      "           8       0.26      0.39      0.31        87\n",
      "           9       0.28      0.39      0.33       110\n",
      "          10       0.16      0.31      0.22        80\n",
      "          11       0.15      0.13      0.14        70\n",
      "          12       0.00      0.00      0.00        36\n",
      "          13       0.00      0.00      0.00        29\n",
      "          14       1.00      0.00      0.00        19\n",
      "          15       1.00      0.00      0.00        14\n",
      "          16       1.00      0.00      0.00        11\n",
      "          17       1.00      0.00      0.00        10\n",
      "          18       1.00      0.00      0.00         5\n",
      "          19       1.00      0.00      0.00         4\n",
      "          20       0.00      0.00      0.00         8\n",
      "          21       1.00      0.00      0.00         3\n",
      "          22       1.00      0.00      0.00         1\n",
      "          23       1.00      0.00      0.00         1\n",
      "\n",
      "    accuracy                           0.22       605\n",
      "   macro avg       0.60      0.09      0.08       605\n",
      "weighted avg       0.30      0.22      0.19       605\n",
      "\n"
     ]
    }
   ],
   "source": [
    "accuracy = accuracy_score(y_val, y_pred)\n",
    "report = classification_report(y_val, y_pred,zero_division=1)\n",
    "\n",
    "print(\"Accuracy:\", accuracy)\n",
    "print(\"Classification Report:\")\n",
    "print(report)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6fdc9036",
   "metadata": {},
   "source": [
    "## Making predictions on test data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "id": "9f187923",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = df_test.drop('Index', axis=1)  \n",
    "X_test_scaled = scaler.transform(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "f86fdf44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# X_test_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "2652c9e8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([10,  9, 13, ..., 10, 10,  9], dtype=int64)"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = logreg.predict(X_test_scaled)\n",
    "predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "id": "a6fbbf8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "## converting array into dataframe\n",
    "target = pd.DataFrame(predictions, columns=['Target'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "id": "0a302c2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission =  pd.concat([df_test['Index'],target],axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "id": "d183d360",
   "metadata": {},
   "outputs": [],
   "source": [
    "submission.to_csv('submission.csv', index=False) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b72a25b7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
